[
    {
        "title": "A tale of two efficient and informative negative sampling distributions",
        "pub_year": "2021",
        "citation": "International conference on machine learning, 2319-2329, 2021"
    },
    {
        "title": "Analyzing log analysis: An empirical study of user log mining",
        "pub_year": "2014",
        "citation": "28th Large Installation System Administration Conference (LISA14), 62-77, 2014"
    },
    {
        "title": "Angular visual hardness",
        "pub_year": "2020",
        "citation": "International Conference on Machine Learning, 1637-1648, 2020"
    },
    {
        "title": "BearLoc: a composable distributed framework for indoor localization systems",
        "pub_year": "2015",
        "citation": "Proceedings of the 2015 Workshop on IoT challenges in Mobile and Industrial …, 2015"
    },
    {
        "title": "CocktailSGD: Fine-tuning foundation models over 500Mbps networks",
        "pub_year": "2023",
        "citation": "International Conference on Machine Learning, 36058-36076, 2023"
    },
    {
        "title": "Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt",
        "pub_year": "2023",
        "citation": "arXiv preprint arXiv:2305.11186, 2023"
    },
    {
        "title": "Decentralized training of foundation models in heterogeneous environments",
        "pub_year": "2022",
        "citation": "Neural Information Processing Systems., 2022"
    },
    {
        "title": "Deja vu: Contextual sparsity for efficient llms at inference time",
        "pub_year": "2023",
        "citation": "International Conference on Machine Learning, 22137-22176, 2023"
    },
    {
        "title": "Densified winner take all (WTA) hashing for sparse datasets",
        "pub_year": "2018",
        "citation": "Uncertainty in artificial intelligence, 2018"
    },
    {
        "title": "Efficient streaming language models with attention sinks",
        "pub_year": "2023",
        "citation": "arXiv preprint arXiv:2309.17453, 2023"
    },
    {
        "title": "Fast Algorithms for a New Relaxation of Optimal Transport",
        "pub_year": "2023",
        "citation": "The Thirty Sixth Annual Conference on Learning Theory, 4831-4862, 2023"
    },
    {
        "title": "Fast and accurate stochastic gradient estimation",
        "pub_year": "2019",
        "citation": "Advances in Neural Information Processing Systems 32, 2019"
    },
    {
        "title": "Fine-tuning language models over slow networks using activation compression with guarantees",
        "pub_year": "2022",
        "citation": "arXiv preprint arXiv:2206.01299, 2022"
    },
    {
        "title": "FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU",
        "pub_year": "2023",
        "citation": "International Conference on Machine Learning, 2023"
    },
    {
        "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding",
        "pub_year": "2024",
        "citation": "arXiv preprint arXiv:2403.04797, 2024"
    },
    {
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
        "pub_year": "2024",
        "citation": "arXiv preprint arXiv:2403.03507, 2024"
    },
    {
        "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference",
        "pub_year": "2024",
        "citation": "arXiv preprint arXiv:2402.09398, 2024"
    },
    {
        "title": "H  O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models",
        "pub_year": "2023",
        "citation": "International Conference on Machine Learning, 2023"
    },
    {
        "title": "Halos: Hashing large output space for cheap inference",
        "pub_year": "2022",
        "citation": "Proceedings of Machine Learning and Systems 4, 110-125, 2022"
    },
    {
        "title": "Hexgen: Generative inference of foundation model over heterogeneous decentralized environment",
        "pub_year": "2023",
        "citation": "arXiv preprint arXiv:2311.11514, 2023"
    },
    {
        "title": "Inrank: Incremental low-rank learning",
        "pub_year": "2023",
        "citation": "arXiv preprint arXiv:2306.11250, 2023"
    },
    {
        "title": "Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention",
        "pub_year": "2023",
        "citation": "arXiv preprint arXiv:2310.00535, 2023"
    },
    {
        "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
        "pub_year": "2024",
        "citation": "arXiv preprint arXiv:2402.02750, 2024"
    },
    {
        "title": "KIVI: Plug-and-play 2bit KV Cache Quantization with Streaming Asymmetric Quantization",
        "citation": ""
    },
    {
        "title": "LLM Inference Unveiled: Survey and Roofline Model Insights",
        "pub_year": "2024",
        "citation": "arXiv preprint arXiv:2402.16363, 2024"
    },
    {
        "title": "Laughing hyena distillery: Extracting compact recurrences from convolutions",
        "pub_year": "2024",
        "citation": "Advances in Neural Information Processing Systems 36, 2024"
    },
    {
        "title": "Learn To be Efficient: Build Structured Sparsity in Large Language Models",
        "pub_year": "2024",
        "citation": "arXiv preprint arXiv:2402.06126, 2024"
    },
    {
        "title": "Locality Sensitive Sampling for Extreme-Scale Optimization and Deep Learning",
        "pub_year": "2020",
        "citation": "Rice University, 2020"
    },
    {
        "title": "Locality sensitive teaching",
        "pub_year": "2021",
        "citation": "Advances in Neural Information Processing Systems 34, 18049-18062, 2021"
    },
    {
        "title": "MONGOOSE: A learnable LSH framework for efficient neural network training",
        "pub_year": "2021",
        "citation": "International Conference on Learning Representations, 2021"
    },
    {
        "title": "Modeling scattering coefficients using self-attentive complex polynomials with image-based representation, 2022",
        "citation": "URL https://arxiv. org/pdf/2301.02747. pdf, 0"
    },
    {
        "title": "Monarch: Expressive structured matrices for efficient and accurate training",
        "pub_year": "2022",
        "citation": "International Conference on Machine Learning, 4690-4721, 2022"
    },
    {
        "title": "On the Similarity between Attention and SVM on the Token Separation and Selection Behavior",
        "pub_year": "2023",
        "citation": ""
    },
    {
        "title": "Pixelated butterfly: Simple and efficient sparse training for neural network models",
        "pub_year": "2022",
        "citation": "International Conference on Learning Representations, 2022"
    },
    {
        "title": "Prompt-prompted Mixture of Experts for Efficient LLM Generation",
        "pub_year": "2024",
        "citation": "arXiv preprint arXiv:2404.01365, 2024"
    },
    {
        "title": "SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-scale Deep Learning Systems",
        "pub_year": "2020",
        "citation": "Proceedings of Machine Learning and System 2, 291--306, 2020"
    },
    {
        "title": "SOLAR: Sparse Orthogonal Learned and Random Embeddings",
        "pub_year": "2021",
        "citation": "International Conference on Learning Representations, 2021"
    },
    {
        "title": "Sample-efficient Surrogate Model for Frequency Response of Linear PDEs using Self-Attentive Complex Polynomials",
        "pub_year": "2023",
        "citation": "arXiv preprint arXiv:2301.02747, 2023"
    },
    {
        "title": "Satellite Images and Deep Learning to Identify Discrepancy in Mailing Addresses with Applications to Census 2020 in Houston",
        "pub_year": "2021",
        "citation": "JSM Proceedings, Statistical Learning and Data Science Section. American …, 2021"
    },
    {
        "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer",
        "pub_year": "2023",
        "citation": "International Conference on Machine Learning, 2023"
    },
    {
        "title": "Scatterbrain: Unifying sparse and low-rank attention",
        "pub_year": "2021",
        "citation": "Advances in Neural Information Processing Systems 34, 17413-17426, 2021"
    },
    {
        "title": "Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding",
        "pub_year": "2024",
        "citation": "arXiv preprint arXiv:2402.12374, 2024"
    },
    {
        "title": "Sub-linear privacy-preserving near-neighbor search",
        "pub_year": "2016",
        "citation": "arXiv preprint arXiv:1612.01835, 2016"
    },
    {
        "title": "Towards Structured Sparsity in Transformers for Efficient Inference",
        "pub_year": "2023",
        "citation": "Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023"
    },
    {
        "title": "Unique entity estimation with application to the Syrian conflict",
        "pub_year": "2018",
        "citation": "The Annals of Applied Statistics 12 (2), 1039-1067, 2018"
    }
]